{"cells":[{"cell_type":"code","source":["# Welcome to your new notebook\n","# Type here in the cell editor to add code!\n","from delta.tables import *\n","from pyspark.sql.functions import *\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"a72caaf2-bb5b-4236-a7c4-c86516bada74","statement_id":7,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-06T16:37:39.2578842Z","session_start_time":"2023-06-06T16:37:39.5069455Z","execution_start_time":"2023-06-06T16:37:39.6891863Z","execution_finish_time":"2023-06-06T16:37:40.0825605Z","spark_jobs":{"numbers":{"SUCCEEDED":0,"RUNNING":0,"FAILED":0,"UNKNOWN":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"3290bb8e-3473-4cd4-b738-e706f78c3702"},"text/plain":"StatementMeta(, a72caaf2-bb5b-4236-a7c4-c86516bada74, 7, Finished, Available)"},"metadata":{}}],"execution_count":5,"metadata":{},"id":"170f04ac-748b-48fd-b4a9-24318ee1e407"},{"cell_type":"code","source":["lakehousePath = \"abfss://85bfc254-9abf-46cc-b1fe-943ec35b3460@msit-onelake.dfs.fabric.microsoft.com/c02dea28-20ca-432b-b6e8-39d0be76f540\"\n","tableName = \"Colors\"\n","tableKey = \"ColorID\"\n","dateColumn = \"ValidTo\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"a72caaf2-bb5b-4236-a7c4-c86516bada74","statement_id":8,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-06T16:37:39.2584917Z","session_start_time":null,"execution_start_time":"2023-06-06T16:37:40.3877089Z","execution_finish_time":"2023-06-06T16:37:40.7241358Z","spark_jobs":{"numbers":{"SUCCEEDED":0,"RUNNING":0,"FAILED":0,"UNKNOWN":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"d4e69458-737b-46ad-931e-88fd70cd824c"},"text/plain":"StatementMeta(, a72caaf2-bb5b-4236-a7c4-c86516bada74, 8, Finished, Available)"},"metadata":{}}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":["parameters"]},"id":"f706ebfe-a1fa-4686-90fa-4143d99aa0a7"},{"cell_type":"code","source":["deltaTablePath = f\"{lakehousePath}/Tables/{tableName}\" #fill in your delta table path \n","# print(deltaTablePath)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"a72caaf2-bb5b-4236-a7c4-c86516bada74","statement_id":9,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-06T16:37:39.2606608Z","session_start_time":null,"execution_start_time":"2023-06-06T16:37:41.0525719Z","execution_finish_time":"2023-06-06T16:37:41.4008883Z","spark_jobs":{"numbers":{"SUCCEEDED":0,"RUNNING":0,"FAILED":0,"UNKNOWN":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"6a7fed64-faa9-4795-a23a-39451ba8800a"},"text/plain":"StatementMeta(, a72caaf2-bb5b-4236-a7c4-c86516bada74, 9, Finished, Available)"},"metadata":{}}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"de874859-43ca-4100-abc8-2c7a18b21fd8"},{"cell_type":"markdown","source":["Get maxdate and number of records in table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6978cfbc-902f-4e5e-851d-a580e23c7bf6"},{"cell_type":"code","source":["df = spark.read.format(\"delta\").load(deltaTablePath)\n","maxdate = df.agg(max(dateColumn)).collect()[0][0]\n","rowcount = df.count()\n","# print(maxdate)\n","maxdate_str = maxdate.strftime(\"%Y-%m-%d %H:%M:%S\")\n","result = \"maxdate=\"+maxdate_str +  \"|rowcount=\"+str(rowcount)\n","mssparkutils.notebook.exit(result)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"a72caaf2-bb5b-4236-a7c4-c86516bada74","statement_id":11,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-06T16:37:52.5480162Z","session_start_time":null,"execution_start_time":"2023-06-06T16:37:52.8250692Z","execution_finish_time":"2023-06-06T16:37:54.5417819Z","spark_jobs":{"numbers":{"SUCCEEDED":8,"RUNNING":0,"FAILED":0,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":59,"rowCount":1,"jobId":25,"name":"count at NativeMethodAccessorImpl.java:0","description":"Job group for statement 11:\ndf = spark.read.format(\"delta\").load(deltaTablePath)\nmaxdate = df.agg(max(dateColumn)).collect()[0][0]\nrowcount = df.count()\n# print(maxdate)\nmaxdate_str = maxdate.strftime(\"%Y-%m-%d %H:%M:%S\")\nresult = \"maxdate=\"+maxdate_str +  \"|rowcount=\"+str(rowcount)\nmssparkutils.notebook.exit(result)","submissionTime":"2023-06-06T16:37:54.106GMT","completionTime":"2023-06-06T16:37:54.119GMT","stageIds":[45,44],"jobGroup":"11","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":59,"dataRead":930,"rowCount":37,"jobId":24,"name":"count at NativeMethodAccessorImpl.java:0","description":"Job group for statement 11:\ndf = spark.read.format(\"delta\").load(deltaTablePath)\nmaxdate = df.agg(max(dateColumn)).collect()[0][0]\nrowcount = df.count()\n# print(maxdate)\nmaxdate_str = maxdate.strftime(\"%Y-%m-%d %H:%M:%S\")\nresult = \"maxdate=\"+maxdate_str +  \"|rowcount=\"+str(rowcount)\nmssparkutils.notebook.exit(result)","submissionTime":"2023-06-06T16:37:54.023GMT","completionTime":"2023-06-06T16:37:54.096GMT","stageIds":[43],"jobGroup":"11","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":1550,"rowCount":3,"jobId":23,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 11:\ndf = spark.read.format(\"delta\").load(deltaTablePath)\nmaxdate = df.agg(max(dateColumn)).collect()[0][0]\nrowcount = df.count()\n# print(maxdate)\nmaxdate_str = maxdate.strftime(\"%Y-%m-%d %H:%M:%S\")\nresult = \"maxdate=\"+maxdate_str +  \"|rowcount=\"+str(rowcount)\nmssparkutils.notebook.exit(result): Filtering files for query","submissionTime":"2023-06-06T16:37:53.830GMT","completionTime":"2023-06-06T16:37:53.969GMT","stageIds":[42,41],"jobGroup":"11","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":2853,"rowCount":50,"jobId":22,"name":"count at NativeMethodAccessorImpl.java:0","description":"Job group for statement 11:\ndf = spark.read.format(\"delta\").load(deltaTablePath)\nmaxdate = df.agg(max(dateColumn)).collect()[0][0]\nrowcount = df.count()\n# print(maxdate)\nmaxdate_str = maxdate.strftime(\"%Y-%m-%d %H:%M:%S\")\nresult = \"maxdate=\"+maxdate_str +  \"|rowcount=\"+str(rowcount)\nmssparkutils.notebook.exit(result)","submissionTime":"2023-06-06T16:37:53.717GMT","completionTime":"2023-06-06T16:37:53.741GMT","stageIds":[38,39,40],"jobGroup":"11","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2853,"dataRead":1193,"rowCount":51,"jobId":21,"name":"count at NativeMethodAccessorImpl.java:0","description":"Job group for statement 11:\ndf = spark.read.format(\"delta\").load(deltaTablePath)\nmaxdate = df.agg(max(dateColumn)).collect()[0][0]\nrowcount = df.count()\n# print(maxdate)\nmaxdate_str = maxdate.strftime(\"%Y-%m-%d %H:%M:%S\")\nresult = \"maxdate=\"+maxdate_str +  \"|rowcount=\"+str(rowcount)\nmssparkutils.notebook.exit(result)","submissionTime":"2023-06-06T16:37:53.502GMT","completionTime":"2023-06-06T16:37:53.704GMT","stageIds":[37,36],"jobGroup":"11","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":59,"rowCount":1,"jobId":20,"name":"collect at /tmp/ipykernel_9769/1590918444.py:2","description":"Job group for statement 11:\ndf = spark.read.format(\"delta\").load(deltaTablePath)\nmaxdate = df.agg(max(dateColumn)).collect()[0][0]\nrowcount = df.count()\n# print(maxdate)\nmaxdate_str = maxdate.strftime(\"%Y-%m-%d %H:%M:%S\")\nresult = \"maxdate=\"+maxdate_str +  \"|rowcount=\"+str(rowcount)\nmssparkutils.notebook.exit(result)","submissionTime":"2023-06-06T16:37:53.409GMT","completionTime":"2023-06-06T16:37:53.424GMT","stageIds":[34,35],"jobGroup":"11","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":59,"dataRead":1028,"rowCount":37,"jobId":19,"name":"collect at /tmp/ipykernel_9769/1590918444.py:2","description":"Job group for statement 11:\ndf = spark.read.format(\"delta\").load(deltaTablePath)\nmaxdate = df.agg(max(dateColumn)).collect()[0][0]\nrowcount = df.count()\n# print(maxdate)\nmaxdate_str = maxdate.strftime(\"%Y-%m-%d %H:%M:%S\")\nresult = \"maxdate=\"+maxdate_str +  \"|rowcount=\"+str(rowcount)\nmssparkutils.notebook.exit(result)","submissionTime":"2023-06-06T16:37:53.313GMT","completionTime":"2023-06-06T16:37:53.398GMT","stageIds":[33],"jobGroup":"11","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":1550,"rowCount":3,"jobId":18,"name":"collect at /tmp/ipykernel_9769/1590918444.py:2","description":"Delta: Job group for statement 11:\ndf = spark.read.format(\"delta\").load(deltaTablePath)\nmaxdate = df.agg(max(dateColumn)).collect()[0][0]\nrowcount = df.count()\n# print(maxdate)\nmaxdate_str = maxdate.strftime(\"%Y-%m-%d %H:%M:%S\")\nresult = \"maxdate=\"+maxdate_str +  \"|rowcount=\"+str(rowcount)\nmssparkutils.notebook.exit(result): Filtering files for query","submissionTime":"2023-06-06T16:37:53.119GMT","completionTime":"2023-06-06T16:37:53.267GMT","stageIds":[31,32],"jobGroup":"11","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"c05b4538-9dae-4514-8aee-e78a94e93aca"},"text/plain":"StatementMeta(, a72caaf2-bb5b-4236-a7c4-c86516bada74, 11, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ExitValue: maxdate=9999-12-31 23:59:59|rowcount=36"]}],"execution_count":9,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"9dcaaa6d-6852-494e-8b13-06357a98af5a"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"host":{},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"known_lakehouses":[{"id":"c02dea28-20ca-432b-b6e8-39d0be76f540"}]}}},"nbformat":4,"nbformat_minor":5}